{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of project_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdzl-mhsW8Ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import os\n",
        "import imageio\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import timeit\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYt67v_hYxoq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a65eb5f4-f1a4-4391-e89f-f1a9683b57ec"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkZeNBXhcnxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_FOLDER = \"/content/drive/My Drive/Eye_fixation\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h5VDQS3Y36R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_dir_train = DATA_FOLDER + '/cv2_training_data/'\n",
        "image_train = root_dir_train + 'train_images.txt'\n",
        "fix_train = root_dir_train + 'train_fixations.txt'\n",
        "root_dir_valid = DATA_FOLDER + '/cv2_validation_data/'\n",
        "image_valid = root_dir_valid + 'val_images.txt'\n",
        "fix_valid = root_dir_valid + 'val_fixations.txt'\n",
        "\n",
        "root_dir_test = DATA_FOLDER + '/cv2_testing_data/'\n",
        "test_images = root_dir_test + 'test_images.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NekYc5BfW8Uk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(filename):\n",
        "    lines = []\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file: \n",
        "            line = line.strip() #or some other preprocessing\n",
        "            lines.append(line)\n",
        "    return lines\n",
        "\n",
        "\n",
        "class FixationDataset(Dataset):\n",
        "    def __init__(self, root_dir, image_file, fixation_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_files = read_file(image_file)\n",
        "        self.fixation_files = read_file(fixation_file)\n",
        "        self.transform = transform\n",
        "        assert(len(self.image_files) == len(self.fixation_files))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        image = imageio.imread(img_name)\n",
        "\n",
        "        fix_name = os.path.join(self.root_dir, self.fixation_files[idx])\n",
        "        fix = imageio.imread(fix_name)\n",
        "\n",
        "        sample = {'image': image, 'fixation': fix}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "            \n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SatX99nbW8Un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Rescale():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, sample):\n",
        "        self.image = sample['image'].astype(np.float32) / 255.0\n",
        "        self.fixation = sample['fixation'].astype(np.float32) / 255.0\n",
        "        return {'image': self.image, 'fixation': self.fixation}\n",
        "    \n",
        "class ToTensor():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, sample):\n",
        "        self.image = sample['image'].T\n",
        "        self.image = torch.from_numpy(self.image)\n",
        "        self.fixation = sample['fixation']\n",
        "        self.fixation = np.expand_dims(self.fixation, axis=0)\n",
        "        self.fixation = torch.from_numpy(self.fixation)\n",
        "        return {'image': self.image, 'fixation': self.fixation}\n",
        "    \n",
        "class Normalize():\n",
        "    def __init__(self):\n",
        "        self.mean = np.array([0.485, 0.456, 0.406])\n",
        "        self.std = np.array([0.229, 0.224, 0.225])\n",
        "    def __call__(self, sample):\n",
        "        self.img0 = ((sample['image'][0,:,:] - self.mean[0]) / self.std[0]).reshape(1,224,224)\n",
        "        self.img1 = ((sample['image'][1,:,:] - self.mean[1]) / self.std[1]).reshape(1,224,224)\n",
        "        self.img2 = ((sample['image'][2,:,:] - self.mean[2]) / self.std[2]).reshape(1,224,224)\n",
        "            \n",
        "        self.fixation = sample['fixation']    \n",
        "        self.image = np.vstack((self.img0, self.img1, self.img2))\n",
        "        self.image = torch.from_numpy(self.image)\n",
        "        return {'image': self.image, 'fixation': self.fixation}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi-8fZ5aW8Uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self): #model_dict=None):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        from torchvision import models\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = models.vgg16_bn(pretrained=False).features\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        xb = self.model(xb)\n",
        "        \n",
        "        return xb\n",
        "        \n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.bn7_3 = nn.BatchNorm2d(num_features=512)\n",
        "        self.bn8_1 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn8_2 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn9_1 = nn.BatchNorm2d(num_features=128)\n",
        "        self.bn9_2 = nn.BatchNorm2d(num_features=128)\n",
        "        self.bn10_1 = nn.BatchNorm2d(num_features=64)\n",
        "        self.bn10_2 = nn.BatchNorm2d(num_features=64)\n",
        "        self.drop_layer = nn.Dropout2d(p=0.5)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', \n",
        "                                    align_corners=False)\n",
        "        self.conv7_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv8_1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
        "        self.conv8_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv9_1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.conv9_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.conv10_1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.conv10_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.output = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        xb = F.relu(self.bn7_3(self.conv7_3(xb)))\n",
        "        xb = F.relu(self.bn8_1(self.conv8_1(xb)))\n",
        "        xb = F.relu(self.bn8_2(self.conv8_2(xb)))\n",
        "        xb = self.upsample(xb)\n",
        "        xb = F.relu(self.bn9_1(self.conv9_1(xb)))\n",
        "        xb = F.relu(self.bn9_2(self.conv9_2(xb)))\n",
        "        xb = self.upsample(xb)\n",
        "        xb = F.relu(self.bn10_1(self.conv10_1(xb)))\n",
        "        xb = F.relu(self.bn10_2(self.conv10_2(xb)))\n",
        "        return self.output(xb)\n",
        "        \n",
        "        \n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        inp = self.encoder(inp)\n",
        "        inp = self.decoder(inp)\n",
        "        return inp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt4MQXH2W8Ut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BCELossWithDownsampling():\n",
        "    def __init__(self):\n",
        "        self.downsample = nn.Upsample(scale_factor=1/8, mode='bilinear', \n",
        "                                    align_corners=False) #nn.AvgPool2d(4, stride=4, count_include_pad=False)\n",
        "        self.loss_fcn = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "    def __call__(self, pred, y):\n",
        "        return self.loss_fcn(pred, self.downsample(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2wsfAvZW8Uy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "composed = torchvision.transforms.Compose([Rescale(), ToTensor()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFcJ4RPIW8U1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 32 ## batch_size_train\n",
        "bs_valid = 32\n",
        "train_dl = DataLoader(FixationDataset(root_dir_train, image_train, fix_train, \n",
        "                                           transform=composed), batch_size=bs, shuffle=True)\n",
        "\n",
        "valid_dl = DataLoader(FixationDataset(root_dir_valid, image_valid, fix_valid,\n",
        "                                     transform=composed), batch_size=bs_valid)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zvpblf3bERU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "980c99d8-2717-4f8c-f9de-977390630977"
      },
      "source": [
        "model = Generator()\n",
        "loss_func = BCELossWithDownsampling()\n",
        "MODEL_PATH = DATA_FOLDER + '/eye_fixation_weights.pth'\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using \", device)\n",
        "model.to(device)\n",
        "\n",
        "new_epochs = 100\n",
        "old_epochs = 0\n",
        "lr = 0.0001\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.00001)\n",
        "# scheduler = StepLR(opt, step_size=30, gamma=0.1)\n",
        "\n",
        "# if os.path.exists(MODEL_PATH):\n",
        "#     checkpoint = torch.load(MODEL_PATH)\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "#     opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#     old_epochs = checkpoint['epoch']\n",
        "#     loss = checkpoint['loss']\n",
        "#     print(\"old epochs:\", old_epochs)\n",
        "\n",
        "tot_epochs = new_epochs + old_epochs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using  cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TTr4N47W8U4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3aca76f0-5d69-46b3-d2c8-4a7ba979051a"
      },
      "source": [
        "tot_train_loss = []\n",
        "tot_valid_loss = []\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(new_epochs):\n",
        "    \n",
        "    curr_epoch = epoch + old_epochs + 1\n",
        "    model.train()\n",
        "    start = timeit.default_timer()\n",
        "    print('epoch: [{}/{}]'.format(curr_epoch, tot_epochs))\n",
        "\n",
        "    for i, xb in enumerate(train_dl):\n",
        "        inp = xb['image'].cuda()\n",
        "        fix_maps = xb['fixation'].cuda()\n",
        "\n",
        "        predictions = model(inp)\n",
        "        loss = loss_func(predictions, fix_maps)\n",
        "        \n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        tot_train_loss.append(loss.item())\n",
        "\n",
        "    train_loss = np.mean(tot_train_loss)\n",
        "    train_losses.append(train_loss)\n",
        "    stop = timeit.default_timer()\n",
        "\n",
        "    print('TRAIN - loss:{:.4f}, time: {:.2f}s \\n'.format(train_loss, (stop-start)))\n",
        "\n",
        "    if curr_epoch % 20 == 0:\n",
        "      torch.save({'epoch':curr_epoch,\n",
        "        'model_state_dict':model.state_dict(),\n",
        "        'optimizer_state_dict': opt.state_dict(),\n",
        "        'loss': loss}, MODEL_PATH)\n",
        "\n",
        "    # if curr_epoch == 50:\n",
        "    #   for param_group in opt.param_groups:\n",
        "    #     param_group['lr'] = 0.00001\n",
        "    #     print(\"---------------------L.R CHANGED-------------------\")\n",
        "\n",
        "    if curr_epoch % 10 == 0:\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i, xb in enumerate(valid_dl):\n",
        "\n",
        "          X = xb['image'].to(device)\n",
        "          y = xb['fixation'].to(device)\n",
        "\n",
        "          pred = model(X)\n",
        "          loss = loss_func(pred, y)\n",
        "\n",
        "          tot_valid_loss.append(loss.item())\n",
        "        \n",
        "        valid_loss = np.mean(tot_valid_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "        print('VALIDATION - loss: {:.6f}\\n'.format(valid_loss))\n",
        "\n",
        "    # scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: [1/110]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN - loss:0.4545, time: 4996.54s \n",
            "\n",
            "epoch: [2/110]\n",
            "TRAIN - loss:0.4109, time: 65.72s \n",
            "\n",
            "epoch: [3/110]\n",
            "TRAIN - loss:0.3833, time: 65.33s \n",
            "\n",
            "epoch: [4/110]\n",
            "TRAIN - loss:0.3631, time: 65.54s \n",
            "\n",
            "epoch: [5/110]\n",
            "TRAIN - loss:0.3468, time: 65.88s \n",
            "\n",
            "epoch: [6/110]\n",
            "TRAIN - loss:0.3332, time: 65.27s \n",
            "\n",
            "epoch: [7/110]\n",
            "TRAIN - loss:0.3219, time: 65.45s \n",
            "\n",
            "epoch: [8/110]\n",
            "TRAIN - loss:0.3121, time: 65.60s \n",
            "\n",
            "epoch: [9/110]\n",
            "TRAIN - loss:0.3033, time: 65.53s \n",
            "\n",
            "epoch: [10/110]\n",
            "TRAIN - loss:0.2954, time: 65.96s \n",
            "\n",
            "VALIDATION - loss: 0.228496\n",
            "\n",
            "epoch: [11/110]\n",
            "TRAIN - loss:0.2883, time: 65.70s \n",
            "\n",
            "epoch: [12/110]\n",
            "TRAIN - loss:0.2823, time: 65.36s \n",
            "\n",
            "epoch: [13/110]\n",
            "TRAIN - loss:0.2765, time: 65.79s \n",
            "\n",
            "epoch: [14/110]\n",
            "TRAIN - loss:0.2710, time: 65.47s \n",
            "\n",
            "epoch: [15/110]\n",
            "TRAIN - loss:0.2657, time: 65.93s \n",
            "\n",
            "epoch: [16/110]\n",
            "TRAIN - loss:0.2608, time: 65.40s \n",
            "\n",
            "epoch: [17/110]\n",
            "TRAIN - loss:0.2562, time: 65.59s \n",
            "\n",
            "epoch: [18/110]\n",
            "TRAIN - loss:0.2519, time: 65.32s \n",
            "\n",
            "epoch: [19/110]\n",
            "TRAIN - loss:0.2479, time: 65.82s \n",
            "\n",
            "epoch: [20/110]\n",
            "TRAIN - loss:0.2442, time: 65.41s \n",
            "\n",
            "VALIDATION - loss: 0.220630\n",
            "\n",
            "epoch: [21/110]\n",
            "TRAIN - loss:0.2406, time: 65.61s \n",
            "\n",
            "epoch: [22/110]\n",
            "TRAIN - loss:0.2373, time: 65.55s \n",
            "\n",
            "epoch: [23/110]\n",
            "TRAIN - loss:0.2342, time: 65.49s \n",
            "\n",
            "epoch: [24/110]\n",
            "TRAIN - loss:0.2313, time: 65.84s \n",
            "\n",
            "epoch: [25/110]\n",
            "TRAIN - loss:0.2285, time: 65.46s \n",
            "\n",
            "epoch: [26/110]\n",
            "TRAIN - loss:0.2258, time: 65.63s \n",
            "\n",
            "epoch: [27/110]\n",
            "TRAIN - loss:0.2234, time: 65.67s \n",
            "\n",
            "epoch: [28/110]\n",
            "TRAIN - loss:0.2211, time: 65.36s \n",
            "\n",
            "epoch: [29/110]\n",
            "TRAIN - loss:0.2189, time: 65.97s \n",
            "\n",
            "epoch: [30/110]\n",
            "TRAIN - loss:0.2168, time: 65.61s \n",
            "\n",
            "VALIDATION - loss: 0.217410\n",
            "\n",
            "epoch: [31/110]\n",
            "TRAIN - loss:0.2148, time: 65.41s \n",
            "\n",
            "epoch: [32/110]\n",
            "TRAIN - loss:0.2130, time: 65.45s \n",
            "\n",
            "epoch: [33/110]\n",
            "TRAIN - loss:0.2112, time: 65.99s \n",
            "\n",
            "epoch: [34/110]\n",
            "TRAIN - loss:0.2095, time: 65.51s \n",
            "\n",
            "epoch: [35/110]\n",
            "TRAIN - loss:0.2080, time: 65.59s \n",
            "\n",
            "epoch: [36/110]\n",
            "TRAIN - loss:0.2064, time: 65.40s \n",
            "\n",
            "epoch: [37/110]\n",
            "TRAIN - loss:0.2050, time: 65.27s \n",
            "\n",
            "epoch: [38/110]\n",
            "TRAIN - loss:0.2036, time: 65.75s \n",
            "\n",
            "epoch: [39/110]\n",
            "TRAIN - loss:0.2023, time: 65.40s \n",
            "\n",
            "epoch: [40/110]\n",
            "TRAIN - loss:0.2010, time: 65.57s \n",
            "\n",
            "VALIDATION - loss: 0.216666\n",
            "\n",
            "epoch: [41/110]\n",
            "TRAIN - loss:0.1998, time: 65.47s \n",
            "\n",
            "epoch: [42/110]\n",
            "TRAIN - loss:0.1987, time: 65.51s \n",
            "\n",
            "epoch: [43/110]\n",
            "TRAIN - loss:0.1976, time: 65.59s \n",
            "\n",
            "epoch: [44/110]\n",
            "TRAIN - loss:0.1965, time: 65.40s \n",
            "\n",
            "epoch: [45/110]\n",
            "TRAIN - loss:0.1955, time: 65.41s \n",
            "\n",
            "epoch: [46/110]\n",
            "TRAIN - loss:0.1946, time: 65.46s \n",
            "\n",
            "epoch: [47/110]\n",
            "TRAIN - loss:0.1936, time: 65.92s \n",
            "\n",
            "epoch: [48/110]\n",
            "TRAIN - loss:0.1928, time: 65.43s \n",
            "\n",
            "epoch: [49/110]\n",
            "TRAIN - loss:0.1919, time: 65.53s \n",
            "\n",
            "epoch: [50/110]\n",
            "TRAIN - loss:0.1911, time: 65.42s \n",
            "\n",
            "---------------------L.R CHANGED-------------------\n",
            "VALIDATION - loss: 0.215729\n",
            "\n",
            "epoch: [51/110]\n",
            "TRAIN - loss:0.1902, time: 65.58s \n",
            "\n",
            "epoch: [52/110]\n",
            "TRAIN - loss:0.1894, time: 65.79s \n",
            "\n",
            "epoch: [53/110]\n",
            "TRAIN - loss:0.1887, time: 65.45s \n",
            "\n",
            "epoch: [54/110]\n",
            "TRAIN - loss:0.1879, time: 65.42s \n",
            "\n",
            "epoch: [55/110]\n",
            "TRAIN - loss:0.1872, time: 65.52s \n",
            "\n",
            "epoch: [56/110]\n",
            "TRAIN - loss:0.1865, time: 65.49s \n",
            "\n",
            "epoch: [57/110]\n",
            "TRAIN - loss:0.1858, time: 65.78s \n",
            "\n",
            "epoch: [58/110]\n",
            "TRAIN - loss:0.1851, time: 65.57s \n",
            "\n",
            "epoch: [59/110]\n",
            "TRAIN - loss:0.1845, time: 65.25s \n",
            "\n",
            "epoch: [60/110]\n",
            "TRAIN - loss:0.1839, time: 65.36s \n",
            "\n",
            "VALIDATION - loss: 0.215023\n",
            "\n",
            "epoch: [61/110]\n",
            "TRAIN - loss:0.1833, time: 65.89s \n",
            "\n",
            "epoch: [62/110]\n",
            "TRAIN - loss:0.1827, time: 65.32s \n",
            "\n",
            "epoch: [63/110]\n",
            "TRAIN - loss:0.1821, time: 65.53s \n",
            "\n",
            "epoch: [64/110]\n",
            "TRAIN - loss:0.1816, time: 65.37s \n",
            "\n",
            "epoch: [65/110]\n",
            "TRAIN - loss:0.1811, time: 65.45s \n",
            "\n",
            "epoch: [66/110]\n",
            "TRAIN - loss:0.1805, time: 65.81s \n",
            "\n",
            "epoch: [67/110]\n",
            "TRAIN - loss:0.1800, time: 65.47s \n",
            "\n",
            "epoch: [68/110]\n",
            "TRAIN - loss:0.1796, time: 65.28s \n",
            "\n",
            "epoch: [69/110]\n",
            "TRAIN - loss:0.1791, time: 65.23s \n",
            "\n",
            "epoch: [70/110]\n",
            "TRAIN - loss:0.1786, time: 65.25s \n",
            "\n",
            "VALIDATION - loss: 0.214528\n",
            "\n",
            "epoch: [71/110]\n",
            "TRAIN - loss:0.1782, time: 65.42s \n",
            "\n",
            "epoch: [72/110]\n",
            "TRAIN - loss:0.1777, time: 65.39s \n",
            "\n",
            "epoch: [73/110]\n",
            "TRAIN - loss:0.1773, time: 65.42s \n",
            "\n",
            "epoch: [74/110]\n",
            "TRAIN - loss:0.1769, time: 65.35s \n",
            "\n",
            "epoch: [75/110]\n",
            "TRAIN - loss:0.1765, time: 65.74s \n",
            "\n",
            "epoch: [76/110]\n",
            "TRAIN - loss:0.1761, time: 65.43s \n",
            "\n",
            "epoch: [77/110]\n",
            "TRAIN - loss:0.1757, time: 65.30s \n",
            "\n",
            "epoch: [78/110]\n",
            "TRAIN - loss:0.1754, time: 65.30s \n",
            "\n",
            "epoch: [79/110]\n",
            "TRAIN - loss:0.1750, time: 65.25s \n",
            "\n",
            "epoch: [80/110]\n",
            "TRAIN - loss:0.1747, time: 65.74s \n",
            "\n",
            "VALIDATION - loss: 0.214192\n",
            "\n",
            "epoch: [81/110]\n",
            "TRAIN - loss:0.1743, time: 65.34s \n",
            "\n",
            "epoch: [82/110]\n",
            "TRAIN - loss:0.1740, time: 65.30s \n",
            "\n",
            "epoch: [83/110]\n",
            "TRAIN - loss:0.1736, time: 65.26s \n",
            "\n",
            "epoch: [84/110]\n",
            "TRAIN - loss:0.1733, time: 65.76s \n",
            "\n",
            "epoch: [85/110]\n",
            "TRAIN - loss:0.1730, time: 65.31s \n",
            "\n",
            "epoch: [86/110]\n",
            "TRAIN - loss:0.1727, time: 65.42s \n",
            "\n",
            "epoch: [87/110]\n",
            "TRAIN - loss:0.1724, time: 65.30s \n",
            "\n",
            "epoch: [88/110]\n",
            "TRAIN - loss:0.1721, time: 65.36s \n",
            "\n",
            "epoch: [89/110]\n",
            "TRAIN - loss:0.1718, time: 65.86s \n",
            "\n",
            "epoch: [90/110]\n",
            "TRAIN - loss:0.1716, time: 65.42s \n",
            "\n",
            "VALIDATION - loss: 0.214062\n",
            "\n",
            "epoch: [91/110]\n",
            "TRAIN - loss:0.1713, time: 65.38s \n",
            "\n",
            "epoch: [92/110]\n",
            "TRAIN - loss:0.1710, time: 65.30s \n",
            "\n",
            "epoch: [93/110]\n",
            "TRAIN - loss:0.1707, time: 65.21s \n",
            "\n",
            "epoch: [94/110]\n",
            "TRAIN - loss:0.1705, time: 65.70s \n",
            "\n",
            "epoch: [95/110]\n",
            "TRAIN - loss:0.1702, time: 65.39s \n",
            "\n",
            "epoch: [96/110]\n",
            "TRAIN - loss:0.1700, time: 65.33s \n",
            "\n",
            "epoch: [97/110]\n",
            "TRAIN - loss:0.1698, time: 65.22s \n",
            "\n",
            "epoch: [98/110]\n",
            "TRAIN - loss:0.1695, time: 65.55s \n",
            "\n",
            "epoch: [99/110]\n",
            "TRAIN - loss:0.1693, time: 65.45s \n",
            "\n",
            "epoch: [100/110]\n",
            "TRAIN - loss:0.1691, time: 65.31s \n",
            "\n",
            "VALIDATION - loss: 0.214078\n",
            "\n",
            "epoch: [101/110]\n",
            "TRAIN - loss:0.1688, time: 65.19s \n",
            "\n",
            "epoch: [102/110]\n",
            "TRAIN - loss:0.1686, time: 65.30s \n",
            "\n",
            "epoch: [103/110]\n",
            "TRAIN - loss:0.1684, time: 65.63s \n",
            "\n",
            "epoch: [104/110]\n",
            "TRAIN - loss:0.1682, time: 65.35s \n",
            "\n",
            "epoch: [105/110]\n",
            "TRAIN - loss:0.1680, time: 65.45s \n",
            "\n",
            "epoch: [106/110]\n",
            "TRAIN - loss:0.1678, time: 65.30s \n",
            "\n",
            "epoch: [107/110]\n",
            "TRAIN - loss:0.1676, time: 65.32s \n",
            "\n",
            "epoch: [108/110]\n",
            "TRAIN - loss:0.1674, time: 65.79s \n",
            "\n",
            "epoch: [109/110]\n",
            "TRAIN - loss:0.1672, time: 65.52s \n",
            "\n",
            "epoch: [110/110]\n",
            "TRAIN - loss:0.1670, time: 65.29s \n",
            "\n",
            "VALIDATION - loss: 0.214163\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}